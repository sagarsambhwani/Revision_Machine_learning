{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c4cc51-158b-4f28-bab1-88bd9a0aa401",
   "metadata": {},
   "source": [
    "### 1. What is the difference between supervised, unsupervised, and reinforcement learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c85d69-2fff-4678-acbc-74f1b6893777",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Supervised vs. Unsupervised vs. Reinforcement Learning\n",
    "# Supervised Learning: Uses labeled data to train models (e.g., regression, classification).\n",
    "# Unsupervised Learning: Uses unlabeled data to find hidden patterns (e.g., clustering, PCA).\n",
    "# Reinforcement Learning: Agents learn through interactions with an environment by receiving rewards (e.g., game-playing agents)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eb40d5-e2fd-4c04-83db-81728877c55c",
   "metadata": {},
   "source": [
    "### 2. Define overfitting and underfitting. How can you prevent them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e27658-3087-403d-ab93-4eacd342145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Overfitting vs. Underfitting & Prevention\n",
    "##Overfitting: Model captures noise and fits the training data too well but fails on new data.\n",
    "\n",
    "#Prevention: Regularization, cross-validation, more data, pruning, dropout.\n",
    "\n",
    "##Underfitting: Model is too simple and cannot capture patterns in the data.\n",
    "\n",
    "#Prevention: Use more features, complex models, reduce regularization, train longer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44406f7b-d4c7-4110-87b7-0c560c3c975e",
   "metadata": {},
   "source": [
    "### 3. Explain the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "151262dc-68ae-465b-8e4e-24f791a10cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Bias-Variance Tradeoff\n",
    "#Bias: Error from a model being too simplistic (underfitting).\n",
    "#Variance: Error from the model being too sensitive to training data (overfitting).\n",
    "#Goal: Balance bias and variance to minimize total error for good generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5526b7f-d273-47bb-9c03-ef0f8deb2abb",
   "metadata": {},
   "source": [
    "### 4. What is the curse of dimensionality? How can it affect machine learning models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78e93c98-24c0-46ac-9475-dfaae647a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Curse of Dimensionality & Effects\n",
    "#As the number of features increases, the data becomes sparse, making models harder to train and generalize.\n",
    "\n",
    "##Effects:\n",
    "\n",
    "#   Increased computational cost.\n",
    "#   Risk of overfitting with too many dimensions.\n",
    "#   Distance metrics become less meaningful.\n",
    "\n",
    "##Solution: Dimensionality reduction techniques (e.g., PCA, feature selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22c5829-7ddb-4752-8b8e-f145a3fd038d",
   "metadata": {},
   "source": [
    "### 5. What are the assumptions of linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a49faf6f-3785-4db2-b433-fb1fcfaea460",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Assumptions of Linear Regression\n",
    "#  Linearity: Relationship between predictors and target is linear.\n",
    "#  Independence: Residuals are independent.\n",
    "#  Homoscedasticity: Constant variance of residuals.\n",
    "#  No Multicollinearity: Predictors should not be highly correlated.\n",
    "#  Normality: Residuals follow a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4e2d0d-5346-4422-9eb3-416c76513e00",
   "metadata": {},
   "source": [
    "### 6. How do you handle missing or corrupted data in a dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4421a14e-93c4-40e8-8649-c07c2640177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Handling Missing or Corrupted Data\n",
    "## Imputation:\n",
    "#   Mean, median, or mode imputation.\n",
    "#   Predict missing values using models.\n",
    "## Drop Rows/Columns: If the missing data is minimal.\n",
    "## Flag and Fill: Add a missing indicator column.\n",
    "## Advanced Methods: Use algorithms like kNN imputation or multiple imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ecd5d0-b872-49b9-bbe3-a3982adb6ab5",
   "metadata": {},
   "source": [
    "### 7. What is the difference between classification and regression? Give examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "122a2bc4-3d49-408f-b9ee-52180646188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Difference Between Classification and Regression\n",
    "## Classification: Predicts discrete labels (e.g., spam vs. not spam).\n",
    "## Regression: Predicts continuous values (e.g., house prices).\n",
    "# Example: Logistic regression (classification), Linear regression (regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385db7de-7b15-4f3b-b6b1-74d7755959e8",
   "metadata": {},
   "source": [
    "### 8. Explain the working of k-Nearest Neighbors (kNN) and its limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a77b4cd5-490e-4195-992b-bb794126c77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### k-Nearest Neighbors (kNN) Working and Limitations\n",
    "## Working: Predicts the label/class of a sample based on the majority class (or average value) of its k nearest neighbors.\n",
    "## Limitations:\n",
    "#    Slow for large datasets.\n",
    "#    Sensitive to irrelevant features and feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adb26ab-9778-44e8-a106-43879814d53e",
   "metadata": {},
   "source": [
    "### 9. How does a decision tree algorithm split data? What are entropy and Gini index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5ea407b-eeac-47c1-93a8-efe35d1b42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decision Tree Splitting, Entropy, and Gini Index\n",
    "## Splitting: At each node, the algorithm selects the feature that best splits the data into pure subsets.\n",
    "\n",
    "## Entropy: Measures impurity or randomness.\n",
    "\n",
    "## Gini Index: Measures the probability of incorrect classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2961667d-5883-4326-936a-d4ffda2a6c0d",
   "metadata": {},
   "source": [
    "### 10. What is the difference between bagging and boosting in ensemble learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a5af482-d6c6-4434-b61e-ee7febcc6657",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Difference Between Bagging and Boosting\n",
    "## Bagging:\n",
    "\n",
    "# Trains multiple models in parallel on random subsets of data (e.g., Random Forest).\n",
    "# Reduces variance.\n",
    "\n",
    "## Boosting:\n",
    "\n",
    "# Trains models sequentially, with each model correcting the errors of the previous one (e.g., AdaBoost, XGBoost).\n",
    "# Reduces bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91a2cbd-294e-450f-9559-632caad427f2",
   "metadata": {},
   "source": [
    "### 11. What are the advantages of using a Random Forest over a single decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aedbc878-c877-4d5c-b3d3-61d5e3eec8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Advantages of Random Forest Over a Single Decision Tree\n",
    "# Reduced Overfitting: Aggregates results from multiple trees, making it more robust.\n",
    "# Improved Accuracy: Averages predictions, reducing variance.\n",
    "# Handles Missing Data: Can work with partial datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ebf8e4-f78c-40ab-83a3-3d796c653229",
   "metadata": {},
   "source": [
    "### 12. Explain how support vector machines (SVM) work. What are kernels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73294c4d-4907-47d4-ba9c-b89b73435669",
   "metadata": {},
   "outputs": [],
   "source": [
    "### How SVM Works and What Are Kernels?\n",
    "# SVM: Finds the hyperplane that best separates data into classes with maximum margin.\n",
    "# Kernels: Functions that map data to higher-dimensional spaces, enabling SVM to work with non-linear data (e.g., RBF kernel, polynomial kernel)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aefb348-5593-4e2d-aafb-673cf889a8d2",
   "metadata": {},
   "source": [
    "### 13. Describe the working of gradient descent. What is the difference between batch, mini-batch, and stochastic gradient descent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07511d2f-3d44-4f9b-b4ea-7b52583773f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Working of Gradient Descent and Its Variants\n",
    "## Gradient Descent: Iteratively updates model parameters to minimize a loss function by moving in the direction of the steepest descent.\n",
    "\n",
    "## Variants:\n",
    "#Batch Gradient Descent: Uses the entire dataset to update parameters.\n",
    "#Stochastic Gradient Descent (SGD): Uses one sample at a time.\n",
    "#Mini-batch Gradient Descent: Uses small batches of data, balancing speed and stability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea85bc91-3e24-47be-97b6-f709372b49c5",
   "metadata": {},
   "source": [
    "### 14. What are some challenges with training deep neural networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45e1ce65-219a-4e21-b86b-bb9cab55e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Challenges with Training Deep Neural Networks\n",
    "## Vanishing/Exploding Gradients: Gradients become too small or too large, hindering learning.\n",
    "## Long Training Times: Requires substantial computation and data.\n",
    "## Overfitting: Large networks may overfit small datasets.\n",
    "## Hyperparameter Tuning: Finding optimal settings is challenging.\n",
    "## Interpretability: Difficult to understand the inner workings of deep models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15622ce4-6394-4b3f-bf7f-04830cf08616",
   "metadata": {},
   "source": [
    "### 15. What is the difference between precision, recall, and F1-score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38501caf-c8fb-44f3-9e0b-fdfa0f11fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Difference Between Precision, Recall, and F1-Score\n",
    "\n",
    "## Precision: Proportion of correctly predicted positive cases among all predicted positives.\n",
    " \n",
    "# Focus: Accuracy of positive predictions.\n",
    "\n",
    "## Recall: Proportion of actual positive cases that were correctly predicted.\n",
    "\n",
    "# Focus: Sensitivity to positive cases.\n",
    "\n",
    "## F1-Score: Harmonic mean of precision and recall, useful when both are equally important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce17d5b-f9cc-423c-8f49-5cc930e54817",
   "metadata": {},
   "source": [
    "### 16. How do you handle class imbalance in a dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fd01e55-7c31-42a0-bf81-929fa5c6dc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Handling Class Imbalance in a Dataset\n",
    "## Resampling Techniques:\n",
    "#    Oversampling (e.g., SMOTE) to increase minority class.\n",
    "#    Undersampling to reduce the majority class.\n",
    "## Use Weighted Models: Assign higher weights to the minority class.\n",
    "## Generate Synthetic Data: Create artificial samples of the minority class.\n",
    "## Use Specific Algorithms: e.g., XGBoost handles imbalanced datasets well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a22c55-a584-4b8c-88e7-8d659219231e",
   "metadata": {},
   "source": [
    "### 17. Explain the ROC curve and AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "803b066e-e776-416b-b025-bce822a60953",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ROC Curve and AUC Score\n",
    "## ROC Curve (Receiver Operating Characteristic):\n",
    "\n",
    "#    Plots True Positive Rate (TPR) vs. False Positive Rate (FPR) at various thresholds.\n",
    "#    Helps visualize the tradeoff between sensitivity (recall) and specificity.\n",
    "## AUC (Area Under the Curve):\n",
    "\n",
    "#   Measures the area under the ROC curve.\n",
    "#   Ranges from 0 to 1:\n",
    "#     1: Perfect model\n",
    "#     0.5: Random guessing\n",
    "#     Higher AUC means better model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b782f99-b06a-4034-af9f-c8bcd8a178c1",
   "metadata": {},
   "source": [
    "### 18. What is cross-validation? Why is it used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a743c24-864c-4467-9aa0-e1ecee9f2e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation: Splits the dataset into multiple folds to train and validate the model on different data subsets.\n",
    "# Types:\n",
    "##  k-Fold Cross-Validation: Divides data into k equal parts and trains k times, each time using a different fold for validation.\n",
    "##  Leave-One-Out (LOO): Uses all but one sample for training.\n",
    "# Why Use It?:\n",
    "##  Helps avoid overfitting and ensures the model generalizes well.\n",
    "##  Provides a more reliable estimate of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c976f2-aa1a-4d6c-b950-34da8ff7d1a9",
   "metadata": {},
   "source": [
    "### 19. What is a confusion matrix, and how do you interpret it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "384d0110-8abe-478e-97e1-25acd9ee106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A confusion matrix is a 2x2 table summarizing the performance of a classification model.\n",
    "\n",
    "#                Predicted Positive\tPredicted Negative\n",
    "#Actual Positive\tTrue Positive (TP)\tFalse Negative (FN)\n",
    "#Actual Negative\tFalse Positive (FP)\tTrue Negative (TN)\n",
    "##TP: Correctly predicted positive cases.\n",
    "##TN: Correctly predicted negative cases.\n",
    "##FP: Incorrectly predicted positive cases (false alarms).\n",
    "##FN: Missed positive cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e580a7a-69d9-4eb3-8018-785ab6f4ff15",
   "metadata": {},
   "source": [
    "### 20. How do you determine whether your model is good enough for production use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c85baad1-0b7a-4558-bb33-0d6a1be0c4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on Multiple Metrics: Use appropriate metrics (e.g., accuracy, precision, recall, F1-score, AUC) based on the problem.\n",
    "# Test on Unseen Data: Ensure performance is consistent on validation/test datasets.\n",
    "# Monitor Generalization: Avoid overfitting by checking for comparable training and test accuracy.\n",
    "# Business Requirements: Ensure the model meets desired performance thresholds for the application.\n",
    "# Scalability and Latency: Check if the model can handle real-time inputs and large data.\n",
    "# Robustness: Validate on edge cases and noisy data.\n",
    "# Monitoring Plan: Set up monitoring for performance drift and re-training mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9a7a06-88e9-4f8a-82cf-e5297fc99b55",
   "metadata": {},
   "source": [
    "### 21. Why is feature scaling important? Which techniques are commonly used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "511357f9-fd92-41f1-abff-d31042ef8444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance:\n",
    "## Ensures features are on a similar scale, improving model convergence and performance (especially for algorithms that rely on distance or gradient calculations, such as kNN, SVM, and neural networks).\n",
    "# Common Techniques:\n",
    "## Standardization (Z-score normalization):\n",
    "#   (Mean = 0, Standard deviation = 1)\n",
    "## Min-Max Scaling:\n",
    "#   (Scales values to [0, 1] range)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba64913-3241-42d3-8e5c-60ecf7eb09ee",
   "metadata": {},
   "source": [
    "### 22. How Does PCA Work? When Would You Use It?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8107f48-55d9-4394-b8c5-9e6b8e8b7a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Working of PCA:\n",
    "\n",
    "# PCA reduces dimensionality by transforming features into new uncorrelated components, ranked by variance.\n",
    "# Steps:\n",
    "#   Standardize the data.\n",
    "#   Compute the covariance matrix.\n",
    "#   Calculate eigenvectors and eigenvalues.\n",
    "#   Select the top k eigenvectors to form principal components.\n",
    "#   Project data onto these components.\n",
    "# When to Use:\n",
    "\n",
    "#  When dealing with high-dimensional data to reduce computational cost and avoid overfitting.\n",
    "#  Helps visualize data in lower dimensions (e.g., 2D, 3D)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3b3649-525f-4abd-ab1d-e73dc6642623",
   "metadata": {},
   "source": [
    "### 23. What is One-Hot Encoding, and Why is it Used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed8c300f-a099-4275-a430-ac9390a63cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition: Converts categorical variables into binary vectors.\n",
    "# Example: For the feature \"Color\" with values {Red, Green, Blue}:\n",
    "\n",
    "#Red → [1, 0, 0]\n",
    "#Green → [0, 1, 0]\n",
    "#Blue → [0, 0, 1]\n",
    "## Why Use It?:\n",
    "\n",
    "#   Allows categorical features to be used in machine learning algorithms that only accept numerical data (e.g., logistic regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bf4feb-d773-42ae-a255-3a9fe7de4a82",
   "metadata": {},
   "source": [
    "### 24. How to Detect and Handle Outliers in a Dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "026cf64d-5f96-49f1-a81f-aa5299ccae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Detection Techniques:\n",
    "\n",
    "#   Box Plots: Identify values outside 1.5 IQR (Interquartile Range).\n",
    "#   Z-Score Method: Values with Z-scores beyond ±3 are considered outliers.\n",
    "#   Isolation Forest or LOF (Local Outlier Factor): Advanced algorithms for detecting outliers.\n",
    "## Handling Methods:\n",
    "\n",
    "#   Remove Outliers: If they are due to data errors.\n",
    "#   Cap Outliers: Set upper/lower bounds.\n",
    "#   Transform Data: Apply log or power transformations.\n",
    "#   Use Robust Models: Algorithms like decision trees or median-based models are less sensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d5792d-2d63-4e94-bdca-0671b3ff402d",
   "metadata": {},
   "source": [
    "### 25. What is the difference between L1 and L2 regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "623d457e-36ec-4b65-8e6f-cfd6351da7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## L1 Regularization (Lasso):\n",
    "\n",
    "#    Adds absolute values of coefficients to the loss function:\n",
    "#    Shrinks some coefficients to zero, resulting in feature selection (sparse model).\n",
    "\n",
    "## L2 Regularization (Ridge):\n",
    "\n",
    "#   Adds squared values of coefficients to the loss function:\n",
    " \n",
    "#   Shrinks coefficients closer to zero but never exactly zero, helping reduce overfitting.\n",
    "\n",
    "## Key Difference: L1 performs feature selection by setting some weights to zero, while L2 prevents overfitting by penalizing large weights without zeroing them out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6b4bbd-1de5-4cae-b90d-e57c5cae22b0",
   "metadata": {},
   "source": [
    "### 26. What is the role of hyperparameter tuning, and how do you perform it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b3673d4-adda-410e-9a5f-5c42a0feb8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Role:\n",
    "\n",
    "#   Optimizes hyperparameters (e.g., learning rate, depth, or number of neighbors) to enhance model performance.\n",
    "#   Unlike parameters (learned during training), hyperparameters are set before training.\n",
    "# How to Perform It:\n",
    "\n",
    "#   Grid Search: Tests all combinations from a specified parameter grid (exhaustive but slow).\n",
    "#   Random Search: Randomly selects parameter combinations (faster and effective).\n",
    "#   Bayesian Optimization / Hyperopt: Uses probabilistic methods to find optimal hyperparameters.\n",
    "#   Automated Tuning (e.g., Optuna): Dynamically explores search space.\n",
    "#   Cross-Validation: Validates each hyperparameter combination to ensure generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6d6b05-66af-4ba2-b284-84b6e323c10f",
   "metadata": {},
   "source": [
    "### 27. How do you approach building a recommendation system?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e825afab-1f65-454d-9982-84367ce85a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types of Recommendation Systems:\n",
    "\n",
    "#   Collaborative Filtering:\n",
    "#      User-based: Recommends items liked by similar users.\n",
    "#      Item-based: Recommends similar items based on user interactions.\n",
    "#   Content-Based Filtering: Uses item attributes to recommend similar products.\n",
    "#   Hybrid Models: Combines collaborative and content-based methods.\n",
    "# Steps:\n",
    "\n",
    "#   Collect data (user behavior, ratings, etc.).\n",
    "#   Clean and preprocess the data.\n",
    "#   Choose an algorithm (e.g., matrix factorization, kNN).\n",
    "#   Train and test the model using metrics like RMSE or precision@k.\n",
    "#   Continuously monitor and improve recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73aa823-e83e-4df0-9330-da4b6f701f72",
   "metadata": {},
   "source": [
    "### 28. What are word embeddings? Explain Word2Vec or GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c55c33d7-276c-4240-bde2-8bfbba4b794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embeddings: Represent words in dense vectors, capturing semantic meanings based on context. Similar words have similar vectors.\n",
    "\n",
    "# Word2Vec:\n",
    "\n",
    "#   Skip-gram: Predicts context words from a target word.\n",
    "#   CBOW (Continuous Bag of Words): Predicts the target word from context words.\n",
    "#   How it Works: Uses a shallow neural network to map words to vector space where words with similar contexts are close.\n",
    "# GloVe (Global Vectors for Word Representation):\n",
    "\n",
    "#   Combines local and global co-occurrence statistics to build word vectors.\n",
    "#   Captures relationships like \"king - man + woman = queen.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a58ec7-7cba-4c3f-89e8-1b3e7ad7ba1f",
   "metadata": {},
   "source": [
    "### 29. What are Generative Adversarial Networks (GANs) and How Do They Work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ea2e893-07db-4dc4-90ed-6a61f976ae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GANs: A class of models used to generate realistic data (e.g., synthetic images, videos).\n",
    "# How They Work:\n",
    "#   Consists of two networks:\n",
    "#     Generator: Creates fake data.\n",
    "#     Discriminator: Distinguishes between real and fake data.\n",
    "#   Both networks compete:\n",
    "#     The generator improves at fooling the discriminator.\n",
    "#     The discriminator improves at detecting fake data.\n",
    "#   Eventually, the generator produces highly realistic data that the discriminator can’t differentiate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa73fa3f-d384-4009-b678-02e9926c901c",
   "metadata": {},
   "source": [
    "### 30. How to Deploy a Machine Learning Model in Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36866eae-88be-415b-9532-0ae3f54a63c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps for Deployment:\n",
    "#   Model Packaging: Serialize the model (e.g., using pickle, ONNX, or TensorFlow SavedModel).\n",
    "Create an API: Serve the model using Flask/FastAPI or deploy on cloud services (AWS Lambda, Google Cloud).\n",
    "Containerization: Use Docker to package the environment and dependencies.\n",
    "Deploy on Cloud/Edge: Use platforms like AWS, Azure, or Kubernetes for scalability.\n",
    "Monitoring: Track model performance with real-time feedback to detect drift or degradation.\n",
    "Automate Re-Training: Set up pipelines to retrain models when new data becomes available."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
